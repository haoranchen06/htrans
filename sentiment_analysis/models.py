#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time: 2021/7/21 9:49
# @Author: chenhr33733
# @File: models.py
# @Software: PyCharm
# @Copyrightï¼šCopyright(c) 2021 Hundsun.com,Inc.All Rights Reserved


import math
import os
import warnings
from dataclasses import dataclass
from typing import Optional, Tuple

from transformers import BertModel
import torch
from torch import nn
from transformers import BertPreTrainedModel
from torch.nn import CrossEntropyLoss, MSELoss
from transformers.modeling_outputs import SequenceClassifierOutput
from transformers import BertTokenizer, BertLMHeadModel, BertConfig, BertForMaskedLM
from focal_loss import FocalLoss
from math import log
from ghm_loss import GHMCELoss


class BertForSCWithWeight(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        weight=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        r"""
        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):
            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,
            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),
            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            if self.num_labels == 1:
                #  We are doing regression
                loss_fct = MSELoss()
                loss = loss_fct(logits.view(-1), labels.view(-1))
            else:
                # loss_fct = CrossEntropyLoss(weight=weight.view(-1, self.num_labels)[0])
                # loss_fct = FocalLoss(alpha=weight.view(-1, self.num_labels)[0], gamma=1)
                loss_fct = GHMCELoss(bins=10, alpha=0.5)
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


if __name__ == '__main__':
    # config = BertConfig.from_pretrained('results/pnn_baseline/checkpoint-500')
    # model = BertForSCWithWeight(config=config)
    pass
